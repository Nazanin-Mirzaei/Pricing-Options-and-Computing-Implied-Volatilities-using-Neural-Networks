import pickle
import numpy as np
np.random.seed(42)
import pandas as pd
from scipy import integrate
import matplotlib.pyplot as plt
import seaborn as sb
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.initializers import GlorotUniform
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential, save_model, load_model
from google.colab import drive
drive.mount('/content/drive/')


with open('/content/drive/MyDrive/Colab Notebooks/df_1000000.pickle', 'rb') as f:
    df = pickle.load(f)

df.dropna(inplace=True)

df.reset_index(drop=True, inplace=True)

df_filtered = df[df['option_price'] < 1]
df_filtered = df_filtered[df_filtered['option_price'] >= 0]

df_filtered.head(10)

# Check if GPU is available

if tf.test.gpu_device_name():
    print('GPU found')
else:
    print("No GPU found")

# Create a TensorFlow session and set it to use the GPU
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)

# Split the dataset into train and test sets

train_df, test_df = train_test_split(df_filtered, test_size=0.1, random_state=55)
train_df, eval_df = train_test_split(train_df, test_size=1/9, random_state=55)

#######################################################

def calc_metrics(y_true, y_pred):

    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return {'mse': mse, 'mae': mae, 'r2': r2}

model = Sequential()
model.add(Dense(600, ))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(Dense(600, ))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(Dense(600, ))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(Dense(600, ))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(1, ))

epochs = 200
batch_size = 1024

initial_learning_rate = 0.01
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100000,
    decay_rate=0.99,
    staircase=True)

steps_per_epoch = batch_size
INIT_LR = 1e-5
MAX_LR = 1e-1

clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=INIT_LR,
    maximal_learning_rate=MAX_LR,
    scale_fn=lambda x: 1/(2.**(x-1)),
    step_size=2 * steps_per_epoch
)

filepath = 'best_model'
checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, mode='min', monitor='val_loss', verbose=1, save_best_only=True)
callbacks_list = [checkpoint]
callbacks=callbacks_list
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=clr), loss='mean_squared_error', )

# Train the model on the GPU

with tf.device('/GPU:0'):
    history = model.fit(
        x = train_df[['m', 'tau', 'r', 'rho', 'kappa', 'theta', 'v0','option_price' ]],
        y = train_df['sigma'],
        validation_data = (eval_df[['m', 'tau', 'r', 'rho', 'kappa', 'theta', 'v0','option_price' ]], eval_df['sigma']),
        batch_size=batch_size,
        epochs=epochs,
        verbose=1,
        callbacks=callbacks_list
    )

# plot lines
plt.plot([i for i in range(1, len(history.history['loss'])+1)], history.history['loss'], label = "training loss")
plt.plot([i for i in range(1, len(history.history['val_loss'])+1)], history.history['val_loss'], label = "validation loss")
plt.legend()
plt.show()

# Evaluate the trained model on the test sets on the GPU
model = tf.keras.models.load_model('best_model')

with tf.device('/GPU:0'):
    wide_test_loss = model.evaluate(
        test_df[['m', 'tau', 'r', 'rho', 'kappa', 'theta', 'v0','option_price']],
        test_df['sigma'],
        verbose=0
    )

wide_test_predictions = model.predict(test_df[['m', 'tau', 'r', 'rho', 'kappa', 'theta', 'v0','option_price']])

# Calculate performance metrics for wide test set

res = calc_metrics(test_df['sigma'], wide_test_predictions)
print(res)

# Decay LR
print(history.history['loss'])
print(history.history['val_loss'])
train_dlr = [0.279300332069397, 0.02086659148335457, 0.02059185691177845, 0.019962353631854057, 0.017194760963320732, 0.015563985332846642, 0.014642780646681786, 0.013978537172079086, 0.013239998370409012, 0.012774510309100151, 0.012551569379866123, 0.012225997634232044, 0.012016495689749718, 0.011901333928108215, 0.011735942214727402, 0.011484460905194283, 0.011444604955613613, 0.011270800605416298, 0.011116394773125648, 0.01105514820665121, 0.01088628452271223, 0.01081124972552061, 0.010643920861184597, 0.010499444790184498, 0.010396198369562626, 0.01028902642428875, 0.010281798429787159, 0.010195625014603138, 0.010122693143785, 0.010056152939796448, 0.010025985538959503, 0.009894737973809242, 0.009860123507678509, 0.009790421463549137, 0.009765872731804848, 0.0097524244338274, 0.0096745565533638, 0.009662672877311707, 0.009577814489603043, 0.009755791164934635, 0.009573288261890411, 0.00949119497090578, 0.00946663785725832, 0.009421315044164658, 0.009368378669023514, 0.00937321875244379, 0.009303713217377663, 0.009328938089311123, 0.009317679330706596, 0.009239811450242996, 0.00923645868897438, 0.009175056591629982, 0.009157671593129635, 0.009146756492555141, 0.009113628417253494, 0.009121863171458244, 0.009134975261986256, 0.009083985351026058, 0.00905943289399147, 0.009050109423696995, 0.009064460173249245, 0.00898781605064869, 0.00908472016453743, 0.009033510461449623, 0.009024347178637981, 0.008947309106588364, 0.008954918943345547, 0.00890137255191803, 0.008904335089027882, 0.008926250040531158, 0.008850624784827232, 0.00883178785443306, 0.008850923739373684, 0.008832339197397232, 0.008807109668850899, 0.008834773674607277, 0.008810754865407944, 0.008795283734798431, 0.00878168735653162, 0.008790547028183937, 0.008731363341212273, 0.00872111413627863, 0.008715139701962471, 0.008785986341536045, 0.008700878359377384, 0.008695192635059357, 0.008702585473656654, 0.008706171065568924, 0.008691016584634781, 0.008634008467197418, 0.008643927052617073, 0.008612645789980888, 0.008668897673487663, 0.008662245236337185, 0.008634758181869984, 0.008618837222456932, 0.008587842807173729, 0.008542448282241821, 0.008561448194086552, 0.008536288514733315, 0.008555709384381771, 0.008573587983846664, 0.008538704365491867, 0.008487158454954624, 0.008518239483237267, 0.008504373021423817, 0.008508669212460518, 0.008503966964781284, 0.008507068268954754, 0.008488383144140244, 0.008468430489301682, 0.008490024134516716, 0.008475755341351032, 0.008453457616269588, 0.008428730070590973, 0.00845998153090477, 0.008434777148067951, 0.00841276254504919, 0.00840697716921568, 0.008383804000914097, 0.008377687074244022, 0.008416050113737583, 0.008395286276936531, 0.008384796790778637, 0.00838053971529007, 0.008376297540962696, 0.00833277590572834, 0.00836016796529293, 0.008356703445315361, 0.008345427922904491, 0.008312838152050972, 0.008343340829014778, 0.008296351879835129, 0.008293516002595425, 0.00829091016203165, 0.008306645788252354, 0.00827700924128294, 0.008284266106784344, 0.00829881988465786, 0.008268783800303936, 0.008266208693385124, 0.008258173242211342, 0.00824078917503357, 0.008221942000091076, 0.008230090141296387, 0.008225486613810062, 0.008258642628788948, 0.00822165422141552, 0.008239162154495716, 0.008195853792130947, 0.008249218575656414, 0.008205978199839592, 0.00820936169475317, 0.008183690719306469, 0.008191941305994987, 0.008186734281480312, 0.008137441240251064, 0.008165937848389149, 0.00816407147794962, 0.008159405551850796, 0.008160879835486412, 0.008155197836458683, 0.008146052248775959, 0.008150557987391949, 0.008140578866004944, 0.00816726591438055, 0.008128955028951168, 0.008095038123428822, 0.008127476088702679, 0.008142114616930485, 0.008114723488688469, 0.008088048547506332, 0.008110923692584038, 0.008080776780843735, 0.008055520243942738, 0.008087173104286194, 0.008059845305979252, 0.008045453578233719, 0.008091275580227375, 0.008066684007644653, 0.00802872609347105, 0.008099516853690147, 0.008017826825380325, 0.008060148917138577, 0.008002563379704952, 0.007988521829247475, 0.00804809108376503, 0.00800884235650301, 0.008032570593059063, 0.007946345955133438, 0.008002186194062233, 0.008004380390048027, 0.008007634431123734, 0.007987864315509796, 0.00796662736684084, 0.007958815433084965, 0.007954573258757591, 0.00797210168093443, 0.007988959550857544, 0.007938085123896599, 0.007934034802019596, 0.007913297042250633, 0.007954644039273262, 0.007928689010441303, 0.00794950406998396, 0.007925044745206833, 0.007877659983932972, 0.007953776977956295, 0.00791311264038086, 0.00786326453089714, 0.007904063910245895, 0.007881843484938145, 0.007875568233430386, 0.007873737253248692, 0.007881446741521358, 0.007848483510315418, 0.007848192937672138, 0.007849992252886295, 0.00785776600241661, 0.007841791026294231, 0.007863998413085938, 0.007887623272836208, 0.007827197201550007, 0.007850507274270058, 0.00781248789280653, 0.007827164605259895, 0.007807618472725153, 0.007823546417057514, 0.00786304660141468, 0.007805586326867342, 0.0077911908738315105, 0.0077856313437223434, 0.007770772557705641, 0.007763196248561144, 0.007788683287799358, 0.007824966683983803, 0.007775553502142429, 0.007755790837109089, 0.007739117369055748, 0.0077695706859230995, 0.007771059405058622, 0.007779174949973822, 0.0077714999206364155, 0.007748896721750498, 0.007738315500319004, 0.007752251345664263, 0.007755248807370663, 0.00775894895195961, 0.00777034554630518, 0.007708956487476826, 0.0077092754654586315, 0.0077429465018212795, 0.0077514974400401115, 0.007718486245721579, 0.007707263808697462, 0.007717100437730551, 0.007737382315099239, 0.007733894046396017, 0.00771158654242754, 0.007703325245529413, 0.007714895065873861, 0.007644810248166323, 0.007695728912949562, 0.007680315524339676, 0.007681034971028566, 0.0076842657290399075, 0.007665671873837709, 0.00767004769295454, 0.007649598643183708, 0.007695530075579882, 0.007670174352824688, 0.007659017574042082, 0.007652448024600744, 0.007628554943948984, 0.007662852760404348, 0.007658099755644798, 0.0076317451894283295, 0.007619858253747225, 0.007665092591196299, 0.0076333992183208466, 0.007602309342473745, 0.007621778640896082, 0.007623415905982256, 0.0076320962980389595, 0.0075947255827486515, 0.007625584490597248, 0.007636818569153547, 0.0076093534007668495, 0.007592067588120699, 0.007594702299684286, 0.007562878075987101, 0.007573872804641724, 0.007560274098068476, 0.007593901362270117, 0.007566298358142376, 0.007558980956673622, 0.007574996445327997, 0.00759504409506917, 0.00761018693447113, 0.007585054263472557]
val_dlr = [0.02047618106007576, 0.020378265529870987, 0.02037755399942398, 0.0243158508092165, 0.03545435518026352, 0.03152255713939667, 0.02773919142782688, 0.02851162664592266, 0.025849338620901108, 0.024841999635100365, 0.025285158306360245, 0.029308386147022247, 0.03280322626233101, 0.029562707990407944, 0.03540909290313721, 0.01771630346775055, 0.0166854877024889, 0.013870147988200188, 0.013859506696462631, 0.01863718405365944, 0.017007773742079735, 0.013425824232399464, 0.011028497479856014, 0.01309396792203188, 0.013023661449551582, 0.01376261655241251, 0.0128313098102808, 0.014984434470534325, 0.013765549287199974, 0.00940798968076706, 0.014510969631373882, 0.016245746985077858, 0.01639208383858204, 0.009686050936579704, 0.01847567781805992, 0.009674913249909878, 0.012660101056098938, 0.0118594104424119, 0.009876882657408714, 0.009112833999097347, 0.00934636127203703, 0.00968756340444088, 0.009564897045493126, 0.010782364755868912, 0.01259547658264637, 0.009081636555492878, 0.012851645238697529, 0.014349264092743397, 0.01068783737719059, 0.008497176691889763, 0.010254914872348309, 0.010153941810131073, 0.00972258672118187, 0.010132186114788055, 0.010308030061423779, 0.010271001607179642, 0.009895638562738895, 0.009353529661893845, 0.00925599504262209, 0.008811861276626587, 0.010896815918385983, 0.011002137325704098, 0.008449775166809559, 0.011954668909311295, 0.011478392407298088, 0.012595685198903084, 0.008156036026775837, 0.010201719589531422, 0.009582119062542915, 0.012187556363642216, 0.00787635613232851, 0.00955243967473507, 0.011141316033899784, 0.008662134408950806, 0.010005631484091282, 0.01052144542336464, 0.009853081777691841, 0.009223499335348606, 0.009785627946257591, 0.007967598736286163, 0.009344491176307201, 0.010198269970715046, 0.008562237955629826, 0.008229709230363369, 0.008118709549307823, 0.007581183686852455, 0.008587103337049484, 0.01027989387512207, 0.009541758336126804, 0.010777200572192669, 0.008457398042082787, 0.008680050261318684, 0.007432431448251009, 0.008285137824714184, 0.00858871079981327, 0.007926211692392826, 0.009348882362246513, 0.009927813895046711, 0.008942725136876106, 0.012526372447609901, 0.00798067543655634, 0.008959976956248283, 0.009379283525049686, 0.00801737792789936, 0.007575683761388063, 0.007948334328830242, 0.008607390336692333, 0.010531903244554996, 0.007055862806737423, 0.007308245170861483, 0.008196407929062843, 0.008669002912938595, 0.008021658286452293, 0.008596687577664852, 0.008433901704847813, 0.009082877077162266, 0.009057899005711079, 0.007141791749745607, 0.00792003609240055, 0.00769847771152854, 0.008718607947230339, 0.00865868479013443, 0.008906710892915726, 0.007883254438638687, 0.0075486646965146065, 0.008108783513307571, 0.008303179405629635, 0.00891038402915001, 0.008887426927685738, 0.007966850884258747, 0.009188941679894924, 0.009641554206609726, 0.0075070527382195, 0.008553052321076393, 0.007553402334451675, 0.009764591231942177, 0.00826673861593008, 0.007379708345979452, 0.007619625888764858, 0.009141674265265465, 0.008027994073927402, 0.007343892939388752, 0.008795893751084805, 0.008584192954003811, 0.007668533828109503, 0.007563131395727396, 0.008522333577275276, 0.009744180366396904, 0.008704875595867634, 0.007746606133878231, 0.00881574023514986, 0.009049362502992153, 0.007375491317361593, 0.007364695426076651, 0.007322084158658981, 0.007966297678649426, 0.007415593136101961, 0.007204750552773476, 0.0074231126345694065, 0.007006379775702953, 0.007269817870110273, 0.007285699248313904, 0.0076888143084943295, 0.007998401299118996, 0.009970585815608501, 0.007645755540579557, 0.008375130593776703, 0.008304397575557232, 0.007738230749964714, 0.007807604037225246, 0.007745018694549799, 0.007929031737148762, 0.009104330092668533, 0.00791217852383852, 0.0069064972922205925, 0.007770097348839045, 0.009578927420079708, 0.008411334827542305, 0.0084214573726058, 0.0077141206711530685, 0.007888590916991234, 0.0073723578825592995, 0.007453870493918657, 0.0069897291250526905, 0.007393266074359417, 0.008631311357021332, 0.0067882053554058075, 0.008275391533970833, 0.007652308791875839, 0.007354219444096088, 0.00910535454750061, 0.007900301367044449, 0.007908044382929802, 0.008599492721259594, 0.00695421826094389, 0.0074070109985768795, 0.008620977401733398, 0.007679652888327837, 0.009069284424185753, 0.00835476815700531, 0.007203330285847187, 0.007114366162568331, 0.00915895588696003, 0.0076601835899055, 0.007969498634338379, 0.010542640462517738, 0.00872293021529913, 0.007901127450168133, 0.007592155598104, 0.00922014843672514, 0.010001366958022118, 0.008768685162067413, 0.0071950554847717285, 0.00865168496966362, 0.009039357304573059, 0.008934348821640015, 0.01017752569168806, 0.010517030954360962, 0.007766870781779289, 0.010386975482106209, 0.0076108649373054504, 0.008004847913980484, 0.007903299294412136, 0.009421780705451965, 0.007352303247898817, 0.009402798488736153, 0.007964469492435455, 0.007818507961928844, 0.00883396714925766, 0.008033676072955132, 0.0075071086175739765, 0.009582559578120708, 0.010290298610925674, 0.007544089574366808, 0.0090206079185009, 0.007085825782269239, 0.01014649122953415, 0.007183313835412264, 0.008900400251150131, 0.006703752093017101, 0.008251011371612549, 0.007788109127432108, 0.009494529105722904, 0.0073220981284976006, 0.008894117549061775, 0.007819141261279583, 0.008131859824061394, 0.009851320646703243, 0.0073555544950068, 0.008394782431423664, 0.012189747765660286, 0.00856718048453331, 0.007837217301130295, 0.008885121904313564, 0.009852102026343346, 0.0074804616160690784, 0.009852822870016098, 0.008517595008015633, 0.007985598407685757, 0.00995048601180315, 0.009389113634824753, 0.0099107651039958, 0.010262810625135899, 0.008014047518372536, 0.008634603582322598, 0.009022335521876812, 0.01035323180258274, 0.009596786461770535, 0.00927541870623827, 0.007598953787237406, 0.008647467009723186, 0.008266113698482513, 0.008810646831989288, 0.009061767719686031, 0.009325605817139149, 0.009319803677499294, 0.007618697360157967, 0.007672806270420551, 0.010725041851401329, 0.00986959133297205, 0.008674351498484612, 0.00976878684014082, 0.010171729139983654, 0.007990729063749313, 0.009504296816885471, 0.007700975518673658, 0.008238728158175945, 0.008308586664497852, 0.009284732863307, 0.008837111294269562, 0.00880887545645237, 0.00817098654806614, 0.01012029405683279, 0.009317548014223576, 0.00931417103856802, 0.009271838702261448, 0.009976884350180626, 0.009443766437470913, 0.008695397526025772, 0.008750533685088158]

# Cyclical LR
print(history.history['loss'])
print(history.history['val_loss'])
train_clr = [0.2782786190509796, 0.020899426192045212, 0.02069709636271, 0.02035679668188095, 0.017759736627340317, 0.01573101244866848, 0.014674076810479164, 0.013912656344473362, 0.013268857263028622, 0.012725443579256535, 0.012526541948318481, 0.012190352194011211, 0.011941337957978249, 0.011769519187510014, 0.011688667349517345, 0.011520752683281898, 0.011482862755656242, 0.011208899319171906, 0.01114200334995985, 0.01094843540340662, 0.010829544626176357, 0.010768708772957325, 0.010697961784899235, 0.010541552677750587, 0.01043129712343216, 0.010369823314249516, 0.01022819709032774, 0.010186223313212395, 0.01010037399828434, 0.0100377406924963, 0.009917319752275944, 0.00989997386932373, 0.009821200743317604, 0.009866077452898026, 0.009758423082530499, 0.009752013720571995, 0.0096418596804142, 0.00959909800440073, 0.00957600586116314, 0.009548706002533436, 0.009515705518424511, 0.00938853807747364, 0.009464552626013756, 0.009417190216481686, 0.009440419264137745, 0.009367333725094795, 0.009314969182014465, 0.009300384670495987, 0.009234800934791565, 0.009233728051185608, 0.009246923960745335, 0.009183636866509914, 0.009196542203426361, 0.009202024899423122, 0.009117561392486095, 0.009123992174863815, 0.009119171649217606, 0.009030467830598354, 0.009068327024579048, 0.009082945995032787, 0.009022565558552742, 0.009016540832817554, 0.008978602476418018, 0.009000827558338642, 0.008990857750177383, 0.008931598626077175, 0.00892522744834423, 0.009038674645125866, 0.009086779318749905, 0.008969551883637905, 0.008915423415601254, 0.008876879699528217, 0.008889001794159412, 0.008862115442752838, 0.008882580325007439, 0.008803646080195904, 0.008811572566628456, 0.008789452724158764, 0.008779359981417656, 0.00881530437618494, 0.008740261197090149, 0.00880550965666771, 0.008758878335356712, 0.008723841048777103, 0.008735098876059055, 0.008698215708136559, 0.008739009499549866, 0.008722644299268723, 0.008641286753118038, 0.008668705821037292, 0.008693797513842583, 0.008654375560581684, 0.008647002279758453, 0.008628872223198414, 0.008605246432125568, 0.008643650449812412, 0.008592716418206692, 0.00858644675463438, 0.008612064644694328, 0.008595917373895645, 0.008561795577406883, 0.008536819368600845, 0.00853771809488535, 0.008545493707060814, 0.00853542611002922, 0.00854960922151804, 0.008508805185556412, 0.008517783135175705, 0.008486279286444187, 0.008492150343954563, 0.008479987271130085, 0.008513864129781723, 0.008443135768175125, 0.008476764895021915, 0.008462436497211456, 0.008472375571727753, 0.008451233617961407, 0.008400325663387775, 0.00845632329583168, 0.00843118317425251, 0.008428763598203659, 0.00843638926744461, 0.008426697924733162, 0.008425597101449966, 0.008435488678514957, 0.008371884934604168, 0.008364197798073292, 0.00837745238095522, 0.008354771882295609, 0.008347846567630768, 0.008383709006011486, 0.008418816141784191, 0.008304176852107048, 0.00828950572758913, 0.008377171121537685, 0.008382542990148067, 0.008314136415719986, 0.008318808861076832, 0.00834154337644577, 0.00828793179243803, 0.008332943543791771, 0.008271105587482452, 0.008318522945046425, 0.0082641476765275, 0.008285759016871452, 0.008269491605460644, 0.008266670629382133, 0.00828277226537466, 0.00825659092515707, 0.008253423497080803, 0.008238373324275017, 0.008264017291367054, 0.008256870321929455, 0.008264557458460331, 0.008244408294558525, 0.008223687298595905, 0.00828429777175188, 0.008218375034630299, 0.008209677413105965, 0.008220153860747814, 0.008215849287807941, 0.008183520287275314, 0.008234558627009392, 0.008183486759662628, 0.008146838285028934, 0.008193254470825195, 0.008204582147300243, 0.00815765280276537, 0.008175339549779892, 0.00816610548645258, 0.008135082200169563, 0.008158239535987377, 0.008209560997784138, 0.008124428801238537, 0.008153033442795277, 0.008149492554366589, 0.008137251250445843, 0.008096207864582539, 0.008158983662724495, 0.00811780896037817, 0.008115202188491821, 0.008084378205239773, 0.008093448355793953, 0.00809040479362011, 0.008128765039145947, 0.008097968995571136, 0.008089651353657246, 0.0080804955214262, 0.008138008415699005, 0.008095715194940567, 0.00807290431112051, 0.008055257610976696, 0.008111286908388138, 0.008106857538223267, 0.008033950813114643, 0.008069672621786594, 0.008044568821787834, 0.00805795006453991, 0.00801699236035347, 0.008065295405685902]
val_clr = [0.020546888932585716, 0.020463576540350914, 0.02044651471078396, 0.021433789283037186, 0.03365649655461311, 0.033725906163454056, 0.03164790943264961, 0.02795558050274849, 0.04486285522580147, 0.02111922763288021, 0.03355378285050392, 0.02849809266626835, 0.026260903105139732, 0.01469049695879221, 0.03106624446809292, 0.014869739301502705, 0.019480083137750626, 0.020603280514478683, 0.027072278782725334, 0.01448967307806015, 0.01505119726061821, 0.015611852519214153, 0.01978270523250103, 0.015378118492662907, 0.024559715762734413, 0.011879157274961472, 0.012404645793139935, 0.013165523298084736, 0.016853494569659233, 0.014836112968623638, 0.012226355262100697, 0.015037243254482746, 0.011894848197698593, 0.010996281169354916, 0.00933638121932745, 0.01109700184315443, 0.011150293052196503, 0.014708458445966244, 0.013655010610818863, 0.012275426648557186, 0.00956592708826065, 0.017961446195840836, 0.014619572088122368, 0.010588767006993294, 0.010702317580580711, 0.013458063825964928, 0.009399178437888622, 0.01734658144414425, 0.00996351893991232, 0.008047999814152718, 0.013599890284240246, 0.009088736027479172, 0.009741317480802536, 0.008464665152132511, 0.010268877260386944, 0.008263742551207542, 0.009582869708538055, 0.008979792706668377, 0.012755904346704483, 0.012109801173210144, 0.00878478679805994, 0.009868421591818333, 0.008871810510754585, 0.010201137512922287, 0.009266947396099567, 0.00953863374888897, 0.016076458618044853, 0.015608263202011585, 0.009369865991175175, 0.00839557871222496, 0.00841174740344286, 0.012260268442332745, 0.008838088251650333, 0.008330493234097958, 0.008160469122231007, 0.009582075290381908, 0.0108290184289217, 0.010840963572263718, 0.008198671042919159, 0.007562161423265934, 0.007548969704657793, 0.011653236113488674, 0.010326194576919079, 0.008092802949249744, 0.008815444074571133, 0.009225809015333652, 0.008969617076218128, 0.009998607449233532, 0.009100177325308323, 0.011139213107526302, 0.009283719584345818, 0.007224105764180422, 0.009242952801287174, 0.008797130547463894, 0.007598963100463152, 0.007569214794784784, 0.007612792309373617, 0.010097968392074108, 0.008250468410551548, 0.01038084365427494, 0.008142388425767422, 0.009919666685163975, 0.011441323906183243, 0.008417041040956974, 0.0083896080031991, 0.007738517597317696, 0.008324231021106243, 0.00820835866034031, 0.0104387067258358, 0.007565011736005545, 0.008225174620747566, 0.010403178632259369, 0.00945853628218174, 0.007925708778202534, 0.009328057058155537, 0.010280903428792953, 0.009954561479389668, 0.008352218195796013, 0.009187580086290836, 0.007683764211833477, 0.0082778325304389, 0.009328442625701427, 0.0111321322619915, 0.007976696826517582, 0.007605505175888538, 0.009820673614740372, 0.00743473693728447, 0.008840392343699932, 0.007370195351541042, 0.008721372112631798, 0.008046786300837994, 0.007662007585167885, 0.008739271201193333, 0.008854742161929607, 0.008067173883318901, 0.008638131432235241, 0.007775532081723213, 0.008389463648200035, 0.008469789288938046, 0.007593480404466391, 0.007854620926082134, 0.008232411928474903, 0.010828674770891666, 0.009349019266664982, 0.007654331624507904, 0.009119581431150436, 0.0070371199399232864, 0.008741410449147224, 0.007568645291030407, 0.00782462116330862, 0.009762399829924107, 0.008626261726021767, 0.00807272084057331, 0.009381895884871483, 0.008447849191725254, 0.008623533882200718, 0.007979259826242924, 0.007953592576086521, 0.007525197695940733, 0.007752636447548866, 0.00828816369175911, 0.00832634512335062, 0.00799259077757597, 0.007716933265328407, 0.007508378010243177, 0.008607028983533382, 0.007032137364149094, 0.008787871338427067, 0.007202746346592903, 0.008414143696427345, 0.007333171088248491, 0.00803816132247448, 0.007414610590785742, 0.0075688837096095085, 0.007474674377590418, 0.007766922004520893, 0.007651627529412508, 0.007910420186817646, 0.0076225195080041885, 0.007759603671729565, 0.008608962409198284, 0.007441578898578882, 0.007450941018760204, 0.007723364979028702, 0.006921584717929363, 0.007898393087089062, 0.00772613612934947, 0.007971837185323238, 0.008921119384467602, 0.009233213029801846, 0.008182051591575146, 0.007667860947549343, 0.009118909016251564, 0.00876932218670845, 0.007479528896510601, 0.00820168387144804, 0.007181135006248951, 0.0071958210319280624, 0.008650161325931549, 0.007734791841357946]

# multiple plots
plt.plot([i for i in range(1, len(train_clr)+1)], train_clr, label = "CLR training loss")
plt.plot([i for i in range(1, len(val_clr)+1)], val_clr, label = "CLR validation loss")
plt.plot([i for i in range(1, len(train_dlr)+1)], train_dlr, label = "DLR training loss")
plt.plot([i for i in range(1, len(val_dlr)+1)], val_dlr, label = "DLR validation loss")
plt.legend()
plt.show()

